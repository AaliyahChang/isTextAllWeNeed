{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# This code was prepared by Aaliyah Chang (20231595) for the ELEC 872 Final Project,\n",
        "# with the support of Claude Sonnet 4.5 (https://claude.ai/) for troubleshooting.\n",
        "# =========================\n",
        "# The SOFTMENT architecture used was developed based on the following paper:\n",
        "\n",
        "# [1]Amrita Singh, Preethu Rose Anish, and Smita Ghaisas. 2024. SOFTMENT: Detecting\n",
        "# Mental Health and Wellbeing of Women in the Software Sector. In Companion of the\n",
        "# 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp '24).\n",
        "# Association for Computing Machinery, New York, NY, USA, 405â€“411. https://doi.org/10.1145/3675094.3678493\n",
        "# =========================\n",
        "\n",
        "# The DAIC-WOZ dataset was used for this project and can be accessed/learned about here:\n",
        "\n",
        "# [2]David DeVault, Ron Artstein, Grace Benn, Teresa Dey, Ed Fast, Alesia Gainer,\n",
        "# Kallirroi Georgila, Jon Gratch, Arno Hartholt, Margaux Lhommet, Gale Lucas, Stacy Marsella,\n",
        "# Fabrizio Morbini, Angela Nazarian, Stefan Scherer, Giota Stratou, Apar Suri, David Traum,\n",
        "# Rachel Wood, Yuyu Xu, Albert Rizzo, and Louis-Philippe Morency. 2014.\n",
        "# SimSensei kiosk: a virtual human interviewer for healthcare decision support.\n",
        "# In Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems\n",
        "# (AAMAS â€™14), International Foundation for Autonomous Agents, Paris, France, 1061â€“1068.\n",
        "\n",
        "# [3]Jonathan Gratch, Ron Artstein, Gale M Lucas, Giota Stratou, Stefan Scherer,\n",
        "# Angela Nazarian, Rachel Wood, Jill Boberg, David DeVault, Stacy Marsella, and others.\n",
        "# 2014. The distress analysis interview corpus of human and computer interviews.\n",
        "# In Lrec, Reykjavik, 3123â€“3128.\n",
        "# =============================================================================\n",
        "\n",
        "# =============================================================================\n",
        "# Setup\n",
        "# =============================================================================\n",
        "print(\"=\"*80)\n",
        "print(\"ELEC 872 Project Code\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "!pip install transformers torch pandas numpy matplotlib seaborn tqdm scipy scikit-learn\n",
        "\n",
        "#from google.colab import drive\n",
        "import os, re, torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaModel, RobertaTokenizer, PegasusForConditionalGeneration, PegasusTokenizer\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# # Mount drive\n",
        "# drive.mount('/content/drive')\n",
        "# os.chdir('/content/drive/MyDrive/GradSchool/ELEC872')\n",
        "\n",
        "# # Configuration\n",
        "# DATA_ROOT = \"/content/drive/MyDrive/GradSchool/ELEC872/daicWoz\"\n",
        "# LABELS_DIR = \"/content/drive/MyDrive/GradSchool/ELEC872/labels\"\n",
        "\n",
        "DATA_ROOT = \"data\"\n",
        "LABELS_DIR = \"labels\"\n",
        "\n",
        "TRAIN_LABEL_FILE = os.path.join(LABELS_DIR, \"train_split_Depression_AVEC2017.csv\")\n",
        "DEV_LABEL_FILE = os.path.join(LABELS_DIR, \"dev_split_Depression_AVEC2017.csv\")\n",
        "TEST_LABEL_FILE = os.path.join(LABELS_DIR, \"full_test_split.csv\")\n",
        "\n",
        "OUTPUT_DIR = \"outputs\"\n",
        "FIGURES_DIR = os.path.join(OUTPUT_DIR, \"figures\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "\n",
        "# Hyperparameters\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_EPOCHS = 10\n",
        "MAX_LENGTH = 128\n",
        "CORR_THRESHOLD = 0.85\n",
        "MIN_UTTERANCE_WORDS = 5\n",
        "\n",
        "# Set seeds (randomly chosen, but to )\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "print(f\"âœ“ Configuration loaded\")\n",
        "print(f\"  Device: {DEVICE}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Max length: {MAX_LENGTH} tokens (utterance-level)\")\n",
        "print(f\"  Min utterance: {MIN_UTTERANCE_WORDS} words\")"
      ],
      "metadata": {
        "id": "CmpfJY-PwNkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Function Definitions\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Function Definitions Loaded\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Text normalization\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\[.*?\\]\", \"\", text)  # Remove annotations\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)  # Keep only letters\n",
        "    return text.strip()\n",
        "\n",
        "def load_transcript_utterances(pid, data_root, min_words=5):\n",
        "    \"\"\"\n",
        "    Load transcript and split into utterances (speaker turns)\n",
        "    Returns list of utterances for the participant\n",
        "    \"\"\"\n",
        "    path = os.path.join(data_root, f\"{pid}_P\", f\"{pid}_TRANSCRIPT.csv\")\n",
        "    if not os.path.exists(path):\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(path, sep=None, engine='python')\n",
        "\n",
        "        # Filter participant speech only\n",
        "        df = df[df[\"speaker\"].str.contains(\"Participant\", case=False, na=False)]\n",
        "\n",
        "        utterances = []\n",
        "        for text in df[\"value\"].astype(str):\n",
        "            cleaned = clean_text(text)\n",
        "            word_count = len(cleaned.split())\n",
        "\n",
        "            # Filter short utterances\n",
        "            if word_count >= min_words:\n",
        "                utterances.append(cleaned)\n",
        "\n",
        "        return utterances\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "def select_aus_globally(pids, data_root, corr_threshold=0.85):\n",
        "    \"\"\"Global AU selection with correlation-based pruning\"\"\"\n",
        "    print(\"\\nðŸ”¬ Global AU Selection\")\n",
        "    all_au_data = []\n",
        "\n",
        "    for pid in tqdm(pids, desc=\"  Loading AU files\"):\n",
        "        path = os.path.join(data_root, f\"{pid}_P\", f\"{pid}_CLNF_AUs.txt\")\n",
        "        if not os.path.exists(path):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(path, sep=None, engine='python')\n",
        "            df.columns = df.columns.str.strip().str.replace(' ', '')\n",
        "\n",
        "            # Get AU intensity columns\n",
        "            au_cols = [c for c in df.columns if c.startswith(\"AU\") and c.endswith(\"_r\")]\n",
        "            if not au_cols:\n",
        "                continue\n",
        "\n",
        "            # Filter successful frames\n",
        "            if 'success' in df.columns:\n",
        "                df = df[df['success'] == 1]\n",
        "\n",
        "            if not df.empty:\n",
        "                all_au_data.append(df[au_cols])\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Combine and prune\n",
        "    combined_df = pd.concat(all_au_data, ignore_index=True)\n",
        "    df_au = combined_df.loc[:, combined_df.var() > 1e-6]\n",
        "\n",
        "    print(f\"  Initial AUs: {len(combined_df.columns)} â†’ After variance: {len(df_au.columns)}\")\n",
        "\n",
        "    # Correlation pruning\n",
        "    corr_matrix = df_au.corr().abs()\n",
        "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "    to_drop = set()\n",
        "    for col in upper_tri.columns:\n",
        "        for row in upper_tri.index:\n",
        "            if upper_tri.loc[row, col] > corr_threshold:\n",
        "                if df_au[row].var() < df_au[col].var():\n",
        "                    to_drop.add(row)\n",
        "                else:\n",
        "                    to_drop.add(col)\n",
        "\n",
        "    selected_aus = sorted([au for au in df_au.columns if au not in to_drop])\n",
        "    print(f\"  After pruning (threshold={corr_threshold}): {len(selected_aus)} AUs\")\n",
        "\n",
        "    return selected_aus\n",
        "\n",
        "def load_pooled_au_features(pid, data_root, selected_aus):\n",
        "    \"\"\"Load and pool AU features for a participant\"\"\"\n",
        "    path = os.path.join(data_root, f\"{pid}_P\", f\"{pid}_CLNF_AUs.txt\")\n",
        "    if not os.path.exists(path):\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(path, sep=None, engine='python')\n",
        "        df.columns = df.columns.str.strip().str.replace(' ', '')\n",
        "\n",
        "        # Filter successful frames\n",
        "        if 'success' in df.columns:\n",
        "            df = df[df['success'] == 1]\n",
        "\n",
        "        if df.empty:\n",
        "            return None\n",
        "\n",
        "        # Mean pool across frames\n",
        "        au_features = df[selected_aus].mean().values\n",
        "        return au_features\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def build_utterance_dataset(labels_df, data_root, selected_aus, min_words=5):\n",
        "    \"\"\"\n",
        "    Build dataset at utterance-level\n",
        "    Each row = one utterance with participant-level label and AU features\n",
        "    \"\"\"\n",
        "    records = []\n",
        "\n",
        "    for _, row in tqdm(labels_df.iterrows(), total=len(labels_df), desc=\"  Building dataset\"):\n",
        "        pid = str(int(row[\"Participant_ID\"]))\n",
        "        phq8_score = row[\"PHQ8_Score\"]\n",
        "\n",
        "        # Load utterances\n",
        "        utterances = load_transcript_utterances(pid, data_root, min_words)\n",
        "        if not utterances:\n",
        "            continue\n",
        "\n",
        "        # Load pooled AUs (same for all utterances from this participant)\n",
        "        au_features = load_pooled_au_features(pid, data_root, selected_aus)\n",
        "        if au_features is None:\n",
        "            continue\n",
        "\n",
        "        # Create record for each utterance\n",
        "        for utterance in utterances:\n",
        "            record = {\n",
        "                \"Participant_ID\": pid,\n",
        "                \"utterance\": utterance,\n",
        "                \"PHQ8_Score\": phq8_score\n",
        "            }\n",
        "            # Add AU features\n",
        "            for i, au_name in enumerate(selected_aus):\n",
        "                record[au_name] = au_features[i]\n",
        "\n",
        "            records.append(record)\n",
        "\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "def create_emotion_labels(df):\n",
        "    \"\"\"Create emotion labels from PHQ-8 using tertile split\"\"\"\n",
        "    scores = df['PHQ8_Score'].values\n",
        "    low_thresh = np.percentile(scores, 33)\n",
        "    high_thresh = np.percentile(scores, 67)\n",
        "\n",
        "    labels = []\n",
        "    for score in scores:\n",
        "        if score <= low_thresh:\n",
        "            labels.append(0)  # Positive (low depression)\n",
        "        elif score <= high_thresh:\n",
        "            labels.append(1)  # Neutral (medium)\n",
        "        else:\n",
        "            labels.append(2)  # Negative (high depression)\n",
        "\n",
        "    return np.array(labels), low_thresh, high_thresh\n",
        "\n"
      ],
      "metadata": {
        "id": "aX3Cr9-cwP7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =============================================================================\n",
        "# PHASE 3: Pre-Processing\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Starting Preprocessing\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load label\n",
        "train_labels = pd.read_csv(TRAIN_LABEL_FILE)[[\"Participant_ID\", \"PHQ8_Score\"]]\n",
        "dev_labels = pd.read_csv(DEV_LABEL_FILE)[[\"Participant_ID\", \"PHQ8_Score\"]]\n",
        "test_labels = pd.read_csv(TEST_LABEL_FILE)[[\"Participant_ID\", \"PHQ8_Score\"]]\n",
        "\n",
        "print(f\"  Train: {len(train_labels)} participants\")\n",
        "print(f\"  Dev: {len(dev_labels)} participants\")\n",
        "print(f\"  Test: {len(test_labels)} participants\")\n",
        "\n",
        "# Global AU selection\n",
        "all_pids = (train_labels[\"Participant_ID\"].tolist() +\n",
        "            dev_labels[\"Participant_ID\"].tolist() +\n",
        "            test_labels[\"Participant_ID\"].tolist())\n",
        "\n",
        "au_columns = select_aus_globally(all_pids, DATA_ROOT, CORR_THRESHOLD)\n",
        "\n",
        "# Build utterance-level datasets\n",
        "print(\"\\n Building test, train and dev datasets\")\n",
        "train_df_utterances = build_utterance_dataset(train_labels, DATA_ROOT, au_columns, MIN_UTTERANCE_WORDS)\n",
        "dev_df_utterances = build_utterance_dataset(dev_labels, DATA_ROOT, au_columns, MIN_UTTERANCE_WORDS)\n",
        "test_df_utterances = build_utterance_dataset(test_labels, DATA_ROOT, au_columns, MIN_UTTERANCE_WORDS)\n",
        "\n",
        "print(f\"\\n  Train: {len(train_df_utterances)} utterances from {train_df_utterances['Participant_ID'].nunique()} participants\")\n",
        "print(f\"  Dev: {len(dev_df_utterances)} utterances from {dev_df_utterances['Participant_ID'].nunique()} participants\")\n",
        "print(f\"  Test: {len(test_df_utterances)} utterances from {test_df_utterances['Participant_ID'].nunique()} participants\")\n",
        "\n",
        "# Create emotion labels\n",
        "print(\"\\nCreating emotion labels...\")\n",
        "train_emotion_labels, thresh_low, thresh_high = create_emotion_labels(train_df_utterances)\n",
        "dev_emotion_labels, _, _ = create_emotion_labels(dev_df_utterances)\n",
        "test_emotion_labels, _, _ = create_emotion_labels(test_df_utterances)\n",
        "\n",
        "train_df_utterances['emotion_label'] = train_emotion_labels\n",
        "dev_df_utterances['emotion_label'] = dev_emotion_labels\n",
        "test_df_utterances['emotion_label'] = test_emotion_labels\n",
        "\n",
        "print(f\"  Thresholds: Pos â‰¤ {thresh_low:.1f} | {thresh_low:.1f} < Neu â‰¤ {thresh_high:.1f} | Neg > {thresh_high:.1f}\")\n",
        "\n",
        "for split_name, labels in [('Train', train_emotion_labels),\n",
        "                           ('Dev', dev_emotion_labels),\n",
        "                           ('Test', test_emotion_labels)]:\n",
        "    counts = np.bincount(labels, minlength=3)\n",
        "    print(f\"  {split_name}: Pos={counts[0]}, Neu={counts[1]}, Neg={counts[2]}\")\n",
        "\n",
        "# Data Augmentation\n",
        "print(\"\\nData Augmentation Pegasus Utterance:\")\n",
        "pegasus_model = PegasusForConditionalGeneration.from_pretrained('tuner007/pegasus_paraphrase').to(DEVICE)\n",
        "pegasus_tokenizer = PegasusTokenizer.from_pretrained('tuner007/pegasus_paraphrase')\n",
        "\n",
        "augmented_records = []\n",
        "print(f\"  Paraphrasing {len(train_df_utterances)} training utterances...\")\n",
        "\n",
        "for _, row in tqdm(train_df_utterances.iterrows(), total=len(train_df_utterances), desc=\"  Augmenting\"):\n",
        "    # Generate 1 paraphrase per utterance\n",
        "    inputs = pegasus_tokenizer(\n",
        "        row['utterance'],\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=60,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    outputs = pegasus_model.generate(\n",
        "        **inputs,\n",
        "        num_beams=5,\n",
        "        num_return_sequences=1,\n",
        "        max_length=60\n",
        "    )\n",
        "\n",
        "    paraphrased = pegasus_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Create augmented record\n",
        "    aug_row = row.copy()\n",
        "    aug_row['utterance'] = paraphrased\n",
        "    aug_row['Participant_ID'] = f\"{row['Participant_ID']}_aug\"\n",
        "    augmented_records.append(aug_row)\n",
        "\n",
        "# Combine\n",
        "train_df_augmented = pd.concat([train_df_utterances, pd.DataFrame(augmented_records)], ignore_index=True)\n",
        "print(f\"  Training utterances: {len(train_df_utterances)} â†’ {len(train_df_augmented)}\")\n",
        "\n",
        "# Clean up GPU\n",
        "del pegasus_model, pegasus_tokenizer\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\nâœ“ Preprocessing complete\")\n",
        "print(f\"  Final train utterances: {len(train_df_augmented)}\")\n",
        "print(f\"  AU features: {len(au_columns)}\")\n"
      ],
      "metadata": {
        "id": "YJZr7VqGwdNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# PHASE 4: Dataset classes\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Defining dataset classes:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "\n",
        "class DAICUtteranceDataset(Dataset):\n",
        "    # Change: Added au_mean and au_std as optional arguments\n",
        "    def __init__(self, df, tokenizer, au_cols, au_mean=None, au_std=None):\n",
        "        self.df = df.dropna(subset=['utterance', 'emotion_label']).reset_index(drop=True)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.au_cols = au_cols\n",
        "\n",
        "        # Change: Use the passed-in mean/std instead of calculating from self.df\n",
        "        if au_mean is not None and au_std is not None:\n",
        "            self.df[au_cols] = (self.df[au_cols] - au_mean) / (au_std + 1e-8)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # Tokenize utterance\n",
        "        encoding = self.tokenizer(\n",
        "            str(row[\"utterance\"]),\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=MAX_LENGTH,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"au_features\": torch.tensor(row[self.au_cols].values.astype(np.float32), dtype=torch.float),\n",
        "            \"labels\": torch.tensor(int(row[\"emotion_label\"]), dtype=torch.long),\n",
        "            \"participant_id\": str(row[\"Participant_ID\"]).split('_')[0],  # Remove _aug suffix\n",
        "            \"phq8_score\": torch.tensor(float(row[\"PHQ8_Score\"]), dtype=torch.float)\n",
        "        }\n",
        "\n",
        "class DAICBaselineDataset(Dataset):\n",
        "    \"\"\"Dataset for baseline: pooled transcript per participant\"\"\"\n",
        "    def __init__(self, labels_df, data_root, tokenizer, au_cols):\n",
        "        self.records = []\n",
        "        self.tokenizer = tokenizer\n",
        "        self.au_cols = au_cols\n",
        "\n",
        "        for _, row in labels_df.iterrows():\n",
        "            pid = str(int(row[\"Participant_ID\"]))\n",
        "\n",
        "            # Load and pool all utterances\n",
        "            utterances = load_transcript_utterances(pid, data_root, min_words=1)  # Don't filter for baseline\n",
        "            if not utterances:\n",
        "                continue\n",
        "\n",
        "            pooled_text = \" \".join(utterances)\n",
        "\n",
        "            # Get PHQ-8 threshold label (0=Low, 1=Med, 2=High)\n",
        "            phq8 = row[\"PHQ8_Score\"]\n",
        "            if phq8 <= thresh_low:\n",
        "                label = 0\n",
        "            elif phq8 <= thresh_high:\n",
        "                label = 1\n",
        "            else:\n",
        "                label = 2\n",
        "\n",
        "            self.records.append({\n",
        "                'text': pooled_text,\n",
        "                'label': label,\n",
        "                'phq8': phq8,\n",
        "                'pid': pid\n",
        "            })\n",
        "\n",
        "        self.records = [r for r in self.records if len(r['text']) > 20]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.records)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        record = self.records[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            record['text'],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=512,  # Full transcript\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(record['label'], dtype=torch.long),\n",
        "            \"participant_id\": record['pid']\n",
        "        }\n",
        "\n",
        "print(\"Dataset classes defined\")"
      ],
      "metadata": {
        "id": "i6dGurKKa6-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# PHASE 5: MODEL ARCHITECTURES\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Model Architectures Defined\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
        "    def __init__(self, patience=7, min_delta=0.0, mode='min'):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.mode = mode\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, score):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            return False\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = score < (self.best_score - self.min_delta)\n",
        "        else:\n",
        "            improved = score > (self.best_score + self.min_delta)\n",
        "\n",
        "        if improved:\n",
        "            self.best_score = score\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "        return self.early_stop\n",
        "\n",
        "class ModelCheckpoint:\n",
        "    \"\"\"Save best model based on validation metric\"\"\"\n",
        "    def __init__(self, filepath, mode='min', save_best_only=True):\n",
        "        self.filepath = filepath\n",
        "        self.mode = mode\n",
        "        self.save_best_only = save_best_only\n",
        "        self.best_score = None\n",
        "\n",
        "    def __call__(self, model, score, epoch):\n",
        "        if not self.save_best_only:\n",
        "            self._save(model, epoch)\n",
        "            return True\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self._save(model, epoch)\n",
        "            return True\n",
        "\n",
        "        improved = (score < self.best_score) if self.mode == 'min' else (score > self.best_score)\n",
        "\n",
        "        if improved:\n",
        "            self.best_score = score\n",
        "            self._save(model, epoch)\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _save(self, model, epoch):\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'best_score': self.best_score\n",
        "        }, self.filepath)\n",
        "        print(f\"  â†’ Model saved to {self.filepath}\")\n",
        "\n",
        "def get_scheduler(optimizer, scheduler_type='cosine', num_epochs=10, num_steps_per_epoch=100):\n",
        "    \"\"\"Create learning rate scheduler\"\"\"\n",
        "    if scheduler_type == 'cosine':\n",
        "        return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "    elif scheduler_type == 'step':\n",
        "        return torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
        "    elif scheduler_type == 'reduce_on_plateau':\n",
        "        return torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "    elif scheduler_type == 'warmup_cosine':\n",
        "        def lr_lambda(epoch):\n",
        "            warmup_epochs = 2\n",
        "            if epoch < warmup_epochs:\n",
        "                return (epoch + 1) / warmup_epochs\n",
        "            return 0.5 * (1 + np.cos(np.pi * (epoch - warmup_epochs) / (num_epochs - warmup_epochs)))\n",
        "        return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "print(\"âœ“ Early stopping and checkpoint utilities defined\")\n",
        "def mean_pooling(token_embeddings, attention_mask):\n",
        "    \"\"\"Mean pooling over token embeddings\"\"\"\n",
        "    mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    summed = torch.sum(token_embeddings * mask, dim=1)\n",
        "    counts = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
        "    return summed / counts\n",
        "\n",
        "# Baseline: Direct PHQ-8 Classification\n",
        "class BaselineClassifier(nn.Module):\n",
        "    \"\"\"Baseline: Pooled transcript â†’ Direct PHQ-8 threshold classification\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.bert = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "        self.classifier = nn.Linear(768, 3)  # 3 PHQ-8 thresholds\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, au_features=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled = mean_pooling(outputs.last_hidden_state, attention_mask)\n",
        "        return self.classifier(pooled)\n",
        "\n",
        "# SOFTMENT: Text-Only Emotion Classifier\n",
        "class TextEmotionClassifier(nn.Module):\n",
        "    \"\"\"Utterance â†’ Emotion classification (Pos/Neu/Neg)\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.bert = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "        self.classifier = nn.Linear(768, 3)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, au_features=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled = mean_pooling(outputs.last_hidden_state, attention_mask)\n",
        "        return self.classifier(pooled)\n",
        "\n",
        "# Video-Only for Late Fusion\n",
        "class VideoEmotionClassifier(nn.Module):\n",
        "    def __init__(self, n_au):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(n_au, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, au_features):\n",
        "        return self.model(au_features)\n",
        "\n",
        "# Early Fusion: Concatenation\n",
        "class EarlyFusionConcat(nn.Module):\n",
        "    def __init__(self, n_au):\n",
        "        super().__init__()\n",
        "        self.bert = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "        self.au_proj = nn.Linear(n_au, 128)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(768 + 128, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, au_features):\n",
        "        text_emb = mean_pooling(self.bert(input_ids, attention_mask)[0], attention_mask)\n",
        "        au_emb = torch.relu(self.au_proj(au_features))\n",
        "        combined = torch.cat([text_emb, au_emb], dim=1)\n",
        "        return self.classifier(combined)\n",
        "\n",
        "# Early Fusion: Gated\n",
        "class EarlyFusionGated(nn.Module):\n",
        "    def __init__(self, n_au):\n",
        "        super().__init__()\n",
        "        self.bert = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "        self.au_proj = nn.Linear(n_au, 768)\n",
        "        self.gate = nn.Sequential(nn.Linear(768 * 2, 1), nn.Sigmoid())\n",
        "        self.classifier = nn.Linear(768, 3)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, au_features):\n",
        "        text_emb = mean_pooling(self.bert(input_ids, attention_mask)[0], attention_mask)\n",
        "        au_emb = torch.relu(self.au_proj(au_features))\n",
        "        gate_weight = self.gate(torch.cat([text_emb, au_emb], dim=1))\n",
        "        fused = gate_weight * text_emb + (1 - gate_weight) * au_emb\n",
        "        return self.classifier(fused)\n"
      ],
      "metadata": {
        "id": "OAv3k1kkwtM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Training and Eval Functions\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Training functions defined\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, dev_loader, model_name, is_baseline=False):\n",
        "    model.to(DEVICE)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_f1 = 0\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\"):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch['input_ids'].to(DEVICE)\n",
        "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "            labels = batch['labels'].to(DEVICE)\n",
        "\n",
        "            if is_baseline:\n",
        "                logits = model(input_ids, attention_mask, None)\n",
        "            else:\n",
        "                au_features = batch['au_features'].to(DEVICE)\n",
        "                logits = model(input_ids, attention_mask, au_features)\n",
        "\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_preds = []\n",
        "        val_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in dev_loader:\n",
        "                input_ids = batch['input_ids'].to(DEVICE)\n",
        "                attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "                labels = batch['labels']\n",
        "\n",
        "                if is_baseline:\n",
        "                    logits = model(input_ids, attention_mask, None)\n",
        "                else:\n",
        "                    au_features = batch['au_features'].to(DEVICE)\n",
        "                    logits = model(input_ids, attention_mask, au_features)\n",
        "\n",
        "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                val_preds.extend(preds)\n",
        "                val_labels.extend(labels.numpy())\n",
        "\n",
        "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
        "        val_acc = accuracy_score(val_labels, val_preds)\n",
        "\n",
        "        print(f\"  Loss: {train_loss/len(train_loader):.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
        "\n",
        "        if val_f1 > best_f1:\n",
        "            best_f1 = val_f1\n",
        "            torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, f\"{model_name}_best.pt\"))\n",
        "\n",
        "    # Load best\n",
        "    model.load_state_dict(torch.load(os.path.join(OUTPUT_DIR, f\"{model_name}_best.pt\")))\n",
        "    print(f\"âœ“ Best Val F1: {best_f1:.4f}\")\n",
        "    return model\n",
        "\n",
        "def calculate_participant_mh_scores(model, test_loader, test_df, is_baseline=False):\n",
        "    # Calculate MH-Scores by aggregating predictions per participant\n",
        "    # SOFTMENT formula: MH-Score = sqrt(mean(negative_confidences)) [1]\n",
        "\n",
        "    model.eval()\n",
        "    utterance_results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(DEVICE)\n",
        "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "            participant_ids = batch['participant_id']\n",
        "            phq8_scores = batch['phq8_score']\n",
        "\n",
        "            if is_baseline:\n",
        "                logits = model(input_ids, attention_mask, None)\n",
        "            else:\n",
        "                au_features = batch['au_features'].to(DEVICE)\n",
        "                logits = model(input_ids, attention_mask, au_features)\n",
        "\n",
        "            probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "\n",
        "            for i in range(len(participant_ids)):\n",
        "                utterance_results.append({\n",
        "                    'participant_id': participant_ids[i],\n",
        "                    'phq8_score': phq8_scores[i].item(),\n",
        "                    'negative_conf': probs[i, 2],  # Confidence of Negative class\n",
        "                    'confidences': probs[i]\n",
        "                })\n",
        "\n",
        "    participant_mh_scores = {}\n",
        "    participant_phq8 = {}\n",
        "\n",
        "    for result in utterance_results:\n",
        "        pid = result['participant_id']\n",
        "        if pid not in participant_mh_scores:\n",
        "            participant_mh_scores[pid] = []\n",
        "            participant_phq8[pid] = result['phq8_score']\n",
        "        participant_mh_scores[pid].append(result['negative_conf'])\n",
        "\n",
        "    mh_scores = []\n",
        "    phq8_scores = []\n",
        "\n",
        "    for pid in sorted(participant_mh_scores.keys()):\n",
        "        negative_confs = participant_mh_scores[pid]\n",
        "        mh_score = np.sqrt(np.mean(negative_confs))\n",
        "        mh_scores.append(mh_score)\n",
        "        phq8_scores.append(participant_phq8[pid])\n",
        "\n",
        "    return np.array(mh_scores), np.array(phq8_scores)\n",
        "\n",
        "def evaluate_baseline(model, test_loader):\n",
        "    \"\"\"Evaluate baseline: Direct classification metrics\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(DEVICE)\n",
        "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "            labels = batch['labels']\n",
        "\n",
        "            logits = model(input_ids, attention_mask, None)\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return {\n",
        "        'f1': f1,\n",
        "        'accuracy': acc,\n",
        "        'predictions': all_preds,\n",
        "        'labels': all_labels\n",
        "    }\n",
        "\n",
        "def evaluate_softment(model, test_loader, test_df, model_name, is_baseline=False):\n",
        "    \"\"\"Evaluate SOFTMENT: MH-Score correlation with PHQ-8\"\"\"\n",
        "    mh_scores, phq8_scores = calculate_participant_mh_scores(model, test_loader, test_df, is_baseline)\n",
        "\n",
        "    pearson_r, pearson_p = pearsonr(mh_scores, phq8_scores)\n",
        "    spearman_r, spearman_p = spearmanr(mh_scores, phq8_scores)\n",
        "\n",
        "    mh_scaled = mh_scores * 20\n",
        "    mae = np.mean(np.abs(phq8_scores - mh_scaled))\n",
        "    rmse = np.sqrt(np.mean((phq8_scores - mh_scaled)**2))\n",
        "\n",
        "    results = {\n",
        "        'model': model_name,\n",
        "        'mh_pearson': pearson_r,\n",
        "        'mh_spearman': spearman_r,\n",
        "        'mh_mae': mae,\n",
        "        'mh_rmse': rmse,\n",
        "        'mh_scores': mh_scores,\n",
        "        'phq8_scores': phq8_scores,\n",
        "        'n_participants': len(mh_scores)\n",
        "    }\n",
        "\n",
        "    print(f\"\\n {model_name}\")\n",
        "    print(f\"  Participants: {len(mh_scores)}\")\n",
        "    print(f\"  MH-Score Pearson: {pearson_r:.4f} (p={pearson_p:.4f})\")\n",
        "    print(f\"  MH-Score Spearman: {spearman_r:.4f}\")\n",
        "    print(f\"  MH-Score MAE: {mae:.4f}\")\n",
        "    print(f\"  MH-Score RMSE: {rmse:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def evaluate_late_fusion(text_model, video_model, test_loader, test_df, fusion_weight=0.5):\n",
        "    \"\"\"Late fusion: Combine separate text and video MH-Scores\"\"\"\n",
        "    text_model.eval()\n",
        "    video_model.eval()\n",
        "    utterance_results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(DEVICE)\n",
        "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "            au_features = batch['au_features'].to(DEVICE)\n",
        "            participant_ids = batch['participant_id']\n",
        "            phq8_scores = batch['phq8_score']\n",
        "\n",
        "            text_logits = text_model(input_ids, attention_mask, None)\n",
        "            text_probs = torch.softmax(text_logits, dim=1).cpu().numpy()\n",
        "\n",
        "            video_logits = video_model(None, None, au_features)\n",
        "            video_probs = torch.softmax(video_logits, dim=1).cpu().numpy()\n",
        "\n",
        "            for i in range(len(participant_ids)):\n",
        "                utterance_results.append({\n",
        "                    'participant_id': participant_ids[i],\n",
        "                    'phq8_score': phq8_scores[i].item(),\n",
        "                    'text_neg_conf': text_probs[i, 2],\n",
        "                    'video_neg_conf': video_probs[i, 2]\n",
        "                })\n",
        "\n",
        "    participant_data = {}\n",
        "    for result in utterance_results:\n",
        "        pid = result['participant_id']\n",
        "        if pid not in participant_data:\n",
        "            participant_data[pid] = {\n",
        "                'text_neg_confs': [],\n",
        "                'video_neg_confs': [],\n",
        "                'phq8': result['phq8_score']\n",
        "            }\n",
        "        participant_data[pid]['text_neg_confs'].append(result['text_neg_conf'])\n",
        "        participant_data[pid]['video_neg_confs'].append(result['video_neg_conf'])\n",
        "\n",
        "    mh_scores = []\n",
        "    phq8_scores = []\n",
        "\n",
        "    for pid in sorted(participant_data.keys()):\n",
        "        data = participant_data[pid]\n",
        "        text_mh = np.sqrt(np.mean(data['text_neg_confs']))\n",
        "        video_mh = np.sqrt(np.mean(data['video_neg_confs']))\n",
        "        combined_mh = fusion_weight * text_mh + (1 - fusion_weight) * video_mh\n",
        "\n",
        "        mh_scores.append(combined_mh)\n",
        "        phq8_scores.append(data['phq8'])\n",
        "\n",
        "    mh_scores = np.array(mh_scores)\n",
        "    phq8_scores = np.array(phq8_scores)\n",
        "\n",
        "    pearson_r, _ = pearsonr(mh_scores, phq8_scores)\n",
        "    spearman_r, _ = spearmanr(mh_scores, phq8_scores)\n",
        "    mh_scaled = mh_scores * 20\n",
        "    mae = np.mean(np.abs(phq8_scores - mh_scaled))\n",
        "    rmse = np.sqrt(np.mean((phq8_scores - mh_scaled)**2))\n",
        "\n",
        "    results = {\n",
        "        'model': 'Late Fusion (Text+Video)',\n",
        "        'mh_pearson': pearson_r,\n",
        "        'mh_spearman': spearman_r,\n",
        "        'mh_mae': mae,\n",
        "        'mh_rmse': rmse,\n",
        "        'mh_scores': mh_scores,\n",
        "        'phq8_scores': phq8_scores,\n",
        "        'n_participants': len(mh_scores)\n",
        "    }\n",
        "\n",
        "    print(f\"\\n Late Fusion\")\n",
        "    print(f\"  Fusion weight: {fusion_weight} text, {1-fusion_weight} video\")\n",
        "    print(f\"  MH-Score Pearson: {pearson_r:.4f}\")\n",
        "    print(f\"  MH-Score MAE: {mae:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s-3ts-TLwyBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Experimentation\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Training Time! (Finally)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "train_au_mean = train_df_augmented[au_columns].mean()\n",
        "train_au_std = train_df_augmented[au_columns].std()\n",
        "\n",
        "\n",
        "train_dataset = DAICUtteranceDataset(train_df_augmented, tokenizer, au_columns,\n",
        "                                     au_mean=train_au_mean, au_std=train_au_std)\n",
        "\n",
        "dev_dataset = DAICUtteranceDataset(dev_df_utterances, tokenizer, au_columns,\n",
        "                                   au_mean=train_au_mean, au_std=train_au_std)\n",
        "\n",
        "test_dataset = DAICUtteranceDataset(test_df_utterances, tokenizer, au_columns,\n",
        "                                    au_mean=train_au_mean, au_std=train_au_std)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "baseline_train = DAICBaselineDataset(train_labels, DATA_ROOT, tokenizer, au_columns)\n",
        "baseline_dev = DAICBaselineDataset(dev_labels, DATA_ROOT, tokenizer, au_columns)\n",
        "baseline_test = DAICBaselineDataset(test_labels, DATA_ROOT, tokenizer, au_columns)\n",
        "\n",
        "baseline_train_loader = DataLoader(baseline_train, batch_size=8, shuffle=True)\n",
        "baseline_dev_loader = DataLoader(baseline_dev, batch_size=8)\n",
        "baseline_test_loader = DataLoader(baseline_test, batch_size=8)\n",
        "\n",
        "all_results = []\n",
        "\n",
        "# BASELINE\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BASELINE: Direct PHQ-8 Threshold Classification\")\n",
        "print(\"=\"*80)\n",
        "baseline_model = train_model(BaselineClassifier(), baseline_train_loader, baseline_dev_loader, \"baseline\", is_baseline=True)\n",
        "baseline_results = evaluate_baseline(baseline_model, baseline_test_loader)\n",
        "\n",
        "# RQ1: TEXT-ONLY\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RQ1: TEXT-ONLY SOFTMENT\")\n",
        "print(\"=\"*80)\n",
        "text_model = train_model(TextEmotionClassifier(), train_loader, dev_loader, \"text_softment\")\n",
        "rq1_results = evaluate_softment(text_model, test_loader, test_df_utterances, \"RQ1: Text-Only SOFTMENT\")\n",
        "all_results.append(rq1_results)\n",
        "\n",
        "# RQ2: MULTIMODAL\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RQ2: MULTIMODAL SOFTMENT (Early Fusion - Concatenation)\")\n",
        "print(\"=\"*80)\n",
        "early_concat_model = train_model(EarlyFusionConcat(len(au_columns)), train_loader, dev_loader, \"early_concat\")\n",
        "rq2_results = evaluate_softment(early_concat_model, test_loader, test_df_utterances, \"RQ2: Early Fusion (Concat)\")\n",
        "all_results.append(rq2_results)\n",
        "\n",
        "# RQ3: FUSION COMPARISON\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RQ3: FUSION METHOD COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n Training Early Fusion (Gated)...\")\n",
        "early_gated_model = train_model(EarlyFusionGated(len(au_columns)), train_loader, dev_loader, \"early_gated\")\n",
        "rq3_gated_results = evaluate_softment(early_gated_model, test_loader, test_df_utterances, \"RQ3: Early Fusion (Gated)\")\n",
        "all_results.append(rq3_gated_results)\n",
        "\n",
        "print(\"\\n Training Video Model for Late Fusion...\")\n",
        "video_model = train_model(VideoEmotionClassifier(len(au_columns)), train_loader, dev_loader, \"video_softment\")\n",
        "rq3_late_results = evaluate_late_fusion(text_model, video_model, test_loader, test_df_utterances, fusion_weight=0.5)\n",
        "all_results.append(rq3_late_results)\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Results Summary\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results_df = pd.DataFrame([\n",
        "    {\n",
        "        'Model': r['model'],\n",
        "        'N_Participants': r['n_participants'],\n",
        "        'MH Pearson': r['mh_pearson'],\n",
        "        'MH Spearman': r['mh_spearman'],\n",
        "        'MH MAE': r['mh_mae'],\n",
        "        'MH RMSE': r['mh_rmse']\n",
        "    }\n",
        "    for r in all_results\n",
        "])\n",
        "\n",
        "baseline_row = pd.DataFrame([{\n",
        "    'Model': 'Baseline (Direct Classification)',\n",
        "    'N_Participants': len(baseline_test),\n",
        "    'MH Pearson': '-', 'MH Spearman': '-', 'MH MAE': '-', 'MH RMSE': '-'\n",
        "}])\n",
        "results_df = pd.concat([baseline_row, results_df], ignore_index=True)\n",
        "print(\"\\n\" + results_df.to_string(index=False))\n",
        "\n",
        "results_df.to_csv(os.path.join(OUTPUT_DIR, \"final_results.csv\"), index=False)\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "softment_models = [r['model'] for r in all_results]\n",
        "x_pos = np.arange(len(softment_models))\n",
        "\n",
        "# Pearson\n",
        "pearson_vals = [r['mh_pearson'] for r in all_results]\n",
        "axes[0, 0].bar(x_pos, pearson_vals, color='steelblue', alpha=0.8, edgecolor='black')\n",
        "axes[0, 0].set_xticks(x_pos)\n",
        "axes[0, 0].set_xticklabels(softment_models, rotation=45, ha='right', fontsize=9)\n",
        "axes[0, 0].set_ylabel('Pearson r')\n",
        "axes[0, 0].set_title('MH-Score Correlation with PHQ-8')\n",
        "\n",
        "# MAE\n",
        "mae_vals = [r['mh_mae'] for r in all_results]\n",
        "axes[0, 1].bar(x_pos, mae_vals, color='coral', alpha=0.8, edgecolor='black')\n",
        "axes[0, 1].set_xticks(x_pos)\n",
        "axes[0, 1].set_xticklabels(softment_models, rotation=45, ha='right', fontsize=9)\n",
        "axes[0, 1].set_ylabel('MAE')\n",
        "axes[0, 1].set_title('MH-Score Mean Absolute Error')\n",
        "\n",
        "# Scatter\n",
        "best_idx = np.argmax(pearson_vals)\n",
        "best_mh = all_results[best_idx]['mh_scores']\n",
        "best_phq8 = all_results[best_idx]['phq8_scores']\n",
        "axes[1, 0].scatter(best_phq8, best_mh, alpha=0.6, edgecolors='k')\n",
        "axes[1, 0].plot([0, 1], [0, 1], 'r--', transform=axes[1, 0].transAxes)\n",
        "axes[1, 0].set_xlabel('PHQ-8 Score')\n",
        "axes[1, 0].set_ylabel('MH-Score')\n",
        "axes[1, 0].set_title(f'Best Model: {softment_models[best_idx]}')\n",
        "\n",
        "# Comparison\n",
        "axes[1, 1].bar(['Baseline F1', 'Best SOFTMENT Pearson'], [baseline_results['f1'], max(pearson_vals)], color=['gray', 'green'])\n",
        "axes[1, 1].set_title('Baseline vs SOFTMENT')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, 'final_results.png'), dpi=300)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "RWtXmE4Fx6mc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}